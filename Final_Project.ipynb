{"cells":[{"cell_type":"code","source":["# Databricks restarts the kernal after every pip install call, so we have to install everything at once\n%pip install pymongo[srv] wordcloud vaderSentiment torch transformers"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"026053e3-fa2e-4b96-960a-2a81585f670c","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Python interpreter will be restarted.\nCollecting pymongo[srv]\n  Downloading pymongo-4.3.3-cp37-cp37m-manylinux2014_x86_64.whl (501 kB)\nCollecting wordcloud\n  Downloading wordcloud-1.8.2.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\nCollecting vaderSentiment\n  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\nCollecting torch\n  Downloading torch-1.13.1-cp37-cp37m-manylinux1_x86_64.whl (887.5 MB)\nCollecting transformers\n  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\nCollecting dnspython&lt;3.0.0,&gt;=1.16.0\n  Downloading dnspython-2.3.0-py3-none-any.whl (283 kB)\nCollecting pillow\n  Downloading Pillow-9.4.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\nRequirement already satisfied: matplotlib in /databricks/python3/lib/python3.7/site-packages (from wordcloud) (3.1.3)\nRequirement already satisfied: numpy&gt;=1.6.1 in /databricks/python3/lib/python3.7/site-packages (from wordcloud) (1.18.1)\nRequirement already satisfied: requests in /databricks/python3/lib/python3.7/site-packages (from vaderSentiment) (2.22.0)\nCollecting nvidia-cuda-runtime-cu11==11.7.99; platform_system == &#34;Linux&#34;\n  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\nCollecting nvidia-cudnn-cu11==8.5.0.96; platform_system == &#34;Linux&#34;\n  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\nCollecting typing-extensions\n  Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\nCollecting nvidia-cuda-nvrtc-cu11==11.7.99; platform_system == &#34;Linux&#34;\n  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\nCollecting nvidia-cublas-cu11==11.10.3.66; platform_system == &#34;Linux&#34;\n  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\nCollecting pyyaml&gt;=5.1\n  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\nCollecting huggingface-hub&lt;1.0,&gt;=0.11.0\n  Downloading huggingface_hub-0.13.1-py3-none-any.whl (199 kB)\nCollecting filelock\n  Downloading filelock-3.9.0-py3-none-any.whl (9.7 kB)\nCollecting regex!=2019.12.17\n  Downloading regex-2022.10.31-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (757 kB)\nCollecting packaging&gt;=20.0\n  Downloading packaging-23.0-py3-none-any.whl (42 kB)\nCollecting tqdm&gt;=4.27\n  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\nCollecting tokenizers!=0.11.3,&lt;0.14,&gt;=0.11.1\n  Downloading tokenizers-0.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\nCollecting importlib-metadata; python_version &lt; &#34;3.8&#34;\n  Downloading importlib_metadata-6.0.0-py3-none-any.whl (21 kB)\nRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /databricks/python3/lib/python3.7/site-packages (from matplotlib-&gt;wordcloud) (2.4.6)\nRequirement already satisfied: python-dateutil&gt;=2.1 in /databricks/python3/lib/python3.7/site-packages (from matplotlib-&gt;wordcloud) (2.8.1)\nRequirement already satisfied: kiwisolver&gt;=1.0.1 in /databricks/python3/lib/python3.7/site-packages (from matplotlib-&gt;wordcloud) (1.1.0)\nRequirement already satisfied: cycler&gt;=0.10 in /databricks/python3/lib/python3.7/site-packages (from matplotlib-&gt;wordcloud) (0.10.0)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /databricks/python3/lib/python3.7/site-packages (from requests-&gt;vaderSentiment) (1.25.8)\nRequirement already satisfied: idna&lt;2.9,&gt;=2.5 in /databricks/python3/lib/python3.7/site-packages (from requests-&gt;vaderSentiment) (2.8)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /databricks/python3/lib/python3.7/site-packages (from requests-&gt;vaderSentiment) (2020.6.20)\nRequirement already satisfied: chardet&lt;3.1.0,&gt;=3.0.2 in /usr/lib/python3/dist-packages (from requests-&gt;vaderSentiment) (3.0.4)\nRequirement already satisfied: wheel in /databricks/python3/lib/python3.7/site-packages (from nvidia-cuda-runtime-cu11==11.7.99; platform_system == &#34;Linux&#34;-&gt;torch) (0.34.2)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from nvidia-cuda-runtime-cu11==11.7.99; platform_system == &#34;Linux&#34;-&gt;torch) (45.2.0)\nCollecting zipp&gt;=0.5\n  Downloading zipp-3.15.0-py3-none-any.whl (6.8 kB)\nRequirement already satisfied: six&gt;=1.5 in /databricks/python3/lib/python3.7/site-packages (from python-dateutil&gt;=2.1-&gt;matplotlib-&gt;wordcloud) (1.14.0)\nInstalling collected packages: dnspython, pymongo, pillow, wordcloud, vaderSentiment, nvidia-cuda-runtime-cu11, nvidia-cublas-cu11, nvidia-cudnn-cu11, typing-extensions, nvidia-cuda-nvrtc-cu11, torch, pyyaml, packaging, tqdm, filelock, zipp, importlib-metadata, huggingface-hub, regex, tokenizers, transformers\nSuccessfully installed dnspython-2.3.0 filelock-3.9.0 huggingface-hub-0.13.1 importlib-metadata-6.0.0 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 packaging-23.0 pillow-9.4.0 pymongo-4.3.3 pyyaml-6.0 regex-2022.10.31 tokenizers-0.13.2 torch-1.13.1 tqdm-4.65.0 transformers-4.26.1 typing-extensions-4.5.0 vaderSentiment-3.3.2 wordcloud-1.8.2.2 zipp-3.15.0\nWARNING: You are using pip version 20.0.2; however, version 23.0.1 is available.\nYou should consider upgrading via the &#39;/local_disk0/.ephemeral_nfs/envs/pythonEnv-2a291571-4e29-4c17-92ad-52e5f2995b9e/bin/python -m pip install --upgrade pip&#39; command.\nPython interpreter will be restarted.\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Python interpreter will be restarted.\nCollecting pymongo[srv]\n  Downloading pymongo-4.3.3-cp37-cp37m-manylinux2014_x86_64.whl (501 kB)\nCollecting wordcloud\n  Downloading wordcloud-1.8.2.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\nCollecting vaderSentiment\n  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\nCollecting torch\n  Downloading torch-1.13.1-cp37-cp37m-manylinux1_x86_64.whl (887.5 MB)\nCollecting transformers\n  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\nCollecting dnspython&lt;3.0.0,&gt;=1.16.0\n  Downloading dnspython-2.3.0-py3-none-any.whl (283 kB)\nCollecting pillow\n  Downloading Pillow-9.4.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\nRequirement already satisfied: matplotlib in /databricks/python3/lib/python3.7/site-packages (from wordcloud) (3.1.3)\nRequirement already satisfied: numpy&gt;=1.6.1 in /databricks/python3/lib/python3.7/site-packages (from wordcloud) (1.18.1)\nRequirement already satisfied: requests in /databricks/python3/lib/python3.7/site-packages (from vaderSentiment) (2.22.0)\nCollecting nvidia-cuda-runtime-cu11==11.7.99; platform_system == &#34;Linux&#34;\n  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\nCollecting nvidia-cudnn-cu11==8.5.0.96; platform_system == &#34;Linux&#34;\n  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\nCollecting typing-extensions\n  Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\nCollecting nvidia-cuda-nvrtc-cu11==11.7.99; platform_system == &#34;Linux&#34;\n  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\nCollecting nvidia-cublas-cu11==11.10.3.66; platform_system == &#34;Linux&#34;\n  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\nCollecting pyyaml&gt;=5.1\n  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\nCollecting huggingface-hub&lt;1.0,&gt;=0.11.0\n  Downloading huggingface_hub-0.13.1-py3-none-any.whl (199 kB)\nCollecting filelock\n  Downloading filelock-3.9.0-py3-none-any.whl (9.7 kB)\nCollecting regex!=2019.12.17\n  Downloading regex-2022.10.31-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (757 kB)\nCollecting packaging&gt;=20.0\n  Downloading packaging-23.0-py3-none-any.whl (42 kB)\nCollecting tqdm&gt;=4.27\n  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\nCollecting tokenizers!=0.11.3,&lt;0.14,&gt;=0.11.1\n  Downloading tokenizers-0.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\nCollecting importlib-metadata; python_version &lt; &#34;3.8&#34;\n  Downloading importlib_metadata-6.0.0-py3-none-any.whl (21 kB)\nRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /databricks/python3/lib/python3.7/site-packages (from matplotlib-&gt;wordcloud) (2.4.6)\nRequirement already satisfied: python-dateutil&gt;=2.1 in /databricks/python3/lib/python3.7/site-packages (from matplotlib-&gt;wordcloud) (2.8.1)\nRequirement already satisfied: kiwisolver&gt;=1.0.1 in /databricks/python3/lib/python3.7/site-packages (from matplotlib-&gt;wordcloud) (1.1.0)\nRequirement already satisfied: cycler&gt;=0.10 in /databricks/python3/lib/python3.7/site-packages (from matplotlib-&gt;wordcloud) (0.10.0)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /databricks/python3/lib/python3.7/site-packages (from requests-&gt;vaderSentiment) (1.25.8)\nRequirement already satisfied: idna&lt;2.9,&gt;=2.5 in /databricks/python3/lib/python3.7/site-packages (from requests-&gt;vaderSentiment) (2.8)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /databricks/python3/lib/python3.7/site-packages (from requests-&gt;vaderSentiment) (2020.6.20)\nRequirement already satisfied: chardet&lt;3.1.0,&gt;=3.0.2 in /usr/lib/python3/dist-packages (from requests-&gt;vaderSentiment) (3.0.4)\nRequirement already satisfied: wheel in /databricks/python3/lib/python3.7/site-packages (from nvidia-cuda-runtime-cu11==11.7.99; platform_system == &#34;Linux&#34;-&gt;torch) (0.34.2)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from nvidia-cuda-runtime-cu11==11.7.99; platform_system == &#34;Linux&#34;-&gt;torch) (45.2.0)\nCollecting zipp&gt;=0.5\n  Downloading zipp-3.15.0-py3-none-any.whl (6.8 kB)\nRequirement already satisfied: six&gt;=1.5 in /databricks/python3/lib/python3.7/site-packages (from python-dateutil&gt;=2.1-&gt;matplotlib-&gt;wordcloud) (1.14.0)\nInstalling collected packages: dnspython, pymongo, pillow, wordcloud, vaderSentiment, nvidia-cuda-runtime-cu11, nvidia-cublas-cu11, nvidia-cudnn-cu11, typing-extensions, nvidia-cuda-nvrtc-cu11, torch, pyyaml, packaging, tqdm, filelock, zipp, importlib-metadata, huggingface-hub, regex, tokenizers, transformers\nSuccessfully installed dnspython-2.3.0 filelock-3.9.0 huggingface-hub-0.13.1 importlib-metadata-6.0.0 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 packaging-23.0 pillow-9.4.0 pymongo-4.3.3 pyyaml-6.0 regex-2022.10.31 tokenizers-0.13.2 torch-1.13.1 tqdm-4.65.0 transformers-4.26.1 typing-extensions-4.5.0 vaderSentiment-3.3.2 wordcloud-1.8.2.2 zipp-3.15.0\nWARNING: You are using pip version 20.0.2; however, version 23.0.1 is available.\nYou should consider upgrading via the &#39;/local_disk0/.ephemeral_nfs/envs/pythonEnv-2a291571-4e29-4c17-92ad-52e5f2995b9e/bin/python -m pip install --upgrade pip&#39; command.\nPython interpreter will be restarted.\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["import numpy as np\nimport pandas as pd\nimport pymongo\n\nimport pyspark\nfrom pyspark.sql import Row\nfrom pyspark.sql.functions import col, udf\nfrom pyspark.sql.types import StringType, StructType, StructField\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nimport transformers\nfrom transformers import pipeline\n\nimport vaderSentiment\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"6e4d98f0-a3d8-4dcf-ab96-6c5088342dde","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["username = \"admin\"\npassword = \"goal-diggers\"\ndatabase = \"reviewdb\"\ncollection_name = \"amazon\"\nip_address = \"msds697-goal-diggers.5f8jw.mongodb.net\"\nclient = pymongo.MongoClient(f\"mongodb+srv://{username}:{password}@{ip_address}\")\ndb = client[database]\namazon_collection = db[collection_name]\namazon_collection.find_one()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"3b3fa581-7be3-42bb-9a67-a9d47eefb2d0","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[2]: {&#39;_id&#39;: ObjectId(&#39;63fc65edf6b47c884faf1386&#39;),\n &#39;ASIN&#39;: &#39;B00015VKT4&#39;,\n &#39;reviews&#39;: [{&#39;review&#39;: &#34;My first review but I have to say......About 15 years ago I developed an allergy to nickle (earrings, necklaces, belt buckles, buttons, bracelets!) and stopped wearing most jewelry. I have to tape over my belt buckles or any metal that contacts my skin and was diagnosed as severely allergic by my doc after allergy skin test. About a year ago I got my ears repierced with my daughter so we could have fun wearing jewelry and have tried every kind of hypo allergenic earring I could find: Studex, Sterling Silver 925, Surgical Steel, 14k gold, and had allergic reactions to all of them if worn for even an hour. Studex that my ears were pierced with was the worst! Bleeding, red itchy swollen earlobes a few weeks to heal in between each time I wore earrings! Searched Amazon for an hour to find these and did some research. They work as promised! I had to force them through (holes had partially closed) and had some initial healing for about 2 weeks. Now I&#39;m wearing them 24/7 no problems! It&#39;s a first and really wonderful. I will get more. This size is a bit small for a main earring so I will get the next size up. THANK YOU.&#34;,\n   &#39;rating&#39;: 5.0,\n   &#39;title&#39;: &#39;No irritation for severe nickle allergy sufferer!&#39;},\n  {&#39;review&#39;: &#39;It is refreshing to wear these earrings because they don\\&#39;t make my earlobes swell up. My ears are too sensitive for &#34;sensitive&#34; earrings and I was about ready to give up and resign myself to clip earrings. Fortunately I found these. They are more expensive than other earrings due to the metal, but well worth it. It\\&#39;s like my ears don\\&#39;t even know they\\&#39;re there :) .&#39;,\n   &#39;rating&#39;: 5.0,\n   &#39;title&#39;: &#39;a relief to wear these&#39;},\n  {&#39;review&#39;: &#34;I just had my ears pierced a little over two months ago and I thought the minor discomfort was part of the healing process.  I decided to try these but honestly didn&#39;t expect there to be a difference.  Within hours of wearing these my ears had no discomfort at all.  Again, my ears weren&#39;t red or swollen but just had a very minor ache.  I&#39;d recommend these to people even if it&#39;s not the last resort to wearing earrings.\\n\\nI also really like the small simple style so these were perfect for me anyway.  I&#39;ve found it very difficult to find earrings I like in stores because they all seem so big and overbearing.  I&#39;m sure that I will end up buying more pairs of these in a similar style and definitely give preference to titanium earrings.&#34;,\n   &#39;rating&#39;: 5.0,\n   &#39;title&#39;: &#39;Pleasantly Surprised&#39;},\n  {&#39;review&#39;: &#34;I just got these a few weeks ago (along with a pair of titanium hoops) and love them. I haven&#39;t been able to wear earrings for years because of nickel allergies, and these earrings haven&#39;t given me any trouble whatsoever!&#34;,\n   &#39;rating&#39;: 5.0,\n   &#39;title&#39;: &#39;I can wear earrings again!&#39;},\n  {&#39;review&#39;: &#39;these are the only earrings that have not caused an allergic reaction and they are very pretty too.&#39;,\n   &#39;rating&#39;: 5.0,\n   &#39;title&#39;: &#39;lovely&#39;},\n  {&#39;review&#39;: &#39;I have only owned these earrings a few weeks, but I\\&#39;m quite pleased with them.  Before buying these, I had been completely unable to wear earrings for many years.  My earlobes would swell, turn red, and bleed immediately after inserting even &#34;hypoallergenic&#34; earrings.  I\\&#39;ve tried other brands, such as Simply Whispers, which I really wanted to love, but I simply couldn\\&#39;t wear them.  I\\&#39;d given up and given away most of my earrings and hadn\\&#39;t bought or tried any earrings for a few years.\\n\\nWhen these first arrived, I had trouble inserting them because my earlobes had almost grown closed, and I traumatized my ears a bit.  Over the next several days, I removed them at night, then irritated my ears again when I reinserted them in the morning.  Finally, I decided to just leave them in and allow my ears to heal, and that has worked just fine.  Now my ears have healed, and I haven\\&#39;t bothered to remove the earrings for a couple of weeks.\\n\\nI noticed that in a review for one of the other styles, the reviewer had two complaints: that the ad appeared misleading in that it looked like you were purchasing three pairs of earrings, and that the 3mm size was tiny.  So take note:  you are ordering just one pair of earrings, and the 3mm (mini) size earrings are very small - take a look at a ruler to get an idea - but they provide an understated bit of sparkle.\\n\\nI highly recommend these for people with metal allergies - I\\&#39;m planning to get another pair.&#39;,\n   &#39;rating&#39;: 5.0,\n   &#39;title&#39;: &#39;Earrings I can wear!&#39;},\n  {&#39;review&#39;: &#39;small enough to fit in my cartilage peircing.&#39;,\n   &#39;rating&#39;: 4.0,\n   &#39;title&#39;: &#39;Four Stars&#39;},\n  {&#39;review&#39;: &#39;Wanted some Titanium earrings that I could sleep in, and not have to keep taking on and off...these are great! Love them!&#39;,\n   &#39;rating&#39;: 5.0,\n   &#39;title&#39;: &#39;Very nice earrings&#39;},\n  {&#39;review&#39;: &#39;They are really nice for the price!  Perfect, made out of titanium for those who have allergies to the metals used in mall piercing places!&#39;,\n   &#39;rating&#39;: 5.0,\n   &#39;title&#39;: &#39;got these for my daughter&#39;},\n  {&#39;review&#39;: &#34;They are a good size for my daughter, but they would be too small for an adult.  My daughter cannot wear metal earrings due to a nickel allergy, but she&#39;s fine with these ones and can leave them in for days.  They are pretty for her.&#34;,\n   &#39;rating&#39;: 5.0,\n   &#39;title&#39;: &#39;Good for a child&#39;},\n  {&#39;review&#39;: &#34;My daughter&#39;s ears were pierced 8 months ago and has only been able to comfortably wear her starter earrings. These are the first earrings that she could change to and didn&#39;t irritate her skin in any way.  They are small but she is 11 so appropriate.&#34;,\n   &#39;rating&#39;: 5.0,\n   &#39;title&#39;: &#39;She can finally change her earrings&#39;},\n  {&#39;review&#39;: &#39;Who knows if these Mini Crystal Silver Titanium Earrings won\\&#39;t lose their finish? I bought some silver hoop earrings from Simply Whispers, and the hoops turned black very quickly--I\\&#39;d only worn them a few times. When I emailed Simply Whispers\\&#39; so-called &#34;customer service&#34; about the problem, I got no response, even though I attached pictures that showed the defect.  After several weeks, I mailed the earrings back to them, with an explanatory letter. I thought if they saw the defect for themselves, they would respond. Again, ZERO response. Once they have your money, you can expect NO response from Simply Whispers if the product proves defective.&#39;,\n   &#39;rating&#39;: 1.0,\n   &#39;title&#39;: &#39;Non-Existent Customer Service&#39;},\n  {&#39;review&#39;: &#39;These are the perfect size for my 1 year old daughter.  No worries about metal allergies and they are very pretty, a perfect everyday earring!&#39;,\n   &#39;rating&#39;: 5.0,\n   &#39;title&#39;: &#39;Perfect&#39;},\n  {&#39;review&#39;: &#34;My ears were red for a couple of hours after putting these in because they weren&#39;t use to wearing earrings.  I was actually surprised I could get these earrings in because it had been so long since I wore any due to a reaction to every pair I ever tried. These are nice, subtle, sparkly ear decorations and they don&#39;t make my ears ooze, swell or glow red.  Love them.  Love that I can wear earrings.&#34;,\n   &#39;rating&#39;: 5.0,\n   &#39;title&#39;: &#34;I&#39;m so glad I can finally wear earrings!&#34;},\n  {&#39;review&#39;: &#34;I ordered these knowing full well that they were small and 3mm. They arrived beautifully packaged, sterile, and upon putting them in earring holes that cannot even tolerate 14K solid gold, I felt nothing. I mean, I felt NO irritation, burning, redness, warmth, and I love them. The backs are titanium too, which makes them non-allergenic also. I had a piercing done with titanium earrings (which turned out to be anodized (plated) titanium over surgical steel which had surgical steel (not titanium) backs. The backs irritated my ears immediately. The plating wore off fast while using hydrogen peroxide and antibacterial ointment on the earrings and holes (as they were irritated).\\n\\nWhat a relief to find these earrings. I have now ordered the Blomdahl sleeper hoops 12mm as well and just waiting for those to arrive. I wear these 3mm studs in my upper ear cartilage piercing and lower lobe 2nd piercing. I love the size because I don&#39;t go for big earrings.\\n\\nI strongly recommend these. And just to let others know who are considering other titanium earrings, even grade 23 titanium irritated the heck out of my ears. The Blomdahl earrings are made of grade 1-2 titanium which has NO ALLOYS and therefore do not create allergies. Other titanium used in earrings (grade 23 and grade 5) is listed as hypo-allergenic, but it has additional metals and alloys which can cause sensitivity issues.&#34;,\n   &#39;rating&#39;: 5.0,\n   &#39;title&#39;: &#39;Worth the cost and not looking  back...&#39;},\n  {&#39;review&#39;: &#39;I bought this pair of earrings because I felt my previously pierced ears were starting to close (I only wore earrings sporadically over the last 2 years), and wearing cheap metals were irritating them.  So I bought this pair mainly for the titanium, so my ears can heal.  They did not irritate at all, I wore them continuously for 3 to 4 weeks.  Now I can wear any earring without problem.  I still wear these to bed.&#39;,\n   &#39;rating&#39;: 5.0,\n   &#39;title&#39;: &#39;Did its job.&#39;}]}</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[2]: {&#39;_id&#39;: ObjectId(&#39;63fc65edf6b47c884faf1386&#39;),\n &#39;ASIN&#39;: &#39;B00015VKT4&#39;,\n &#39;reviews&#39;: [{&#39;review&#39;: &#34;My first review but I have to say......About 15 years ago I developed an allergy to nickle (earrings, necklaces, belt buckles, buttons, bracelets!) and stopped wearing most jewelry. I have to tape over my belt buckles or any metal that contacts my skin and was diagnosed as severely allergic by my doc after allergy skin test. About a year ago I got my ears repierced with my daughter so we could have fun wearing jewelry and have tried every kind of hypo allergenic earring I could find: Studex, Sterling Silver 925, Surgical Steel, 14k gold, and had allergic reactions to all of them if worn for even an hour. Studex that my ears were pierced with was the worst! Bleeding, red itchy swollen earlobes a few weeks to heal in between each time I wore earrings! Searched Amazon for an hour to find these and did some research. They work as promised! I had to force them through (holes had partially closed) and had some initial healing for about 2 weeks. Now I&#39;m wearing them 24/7 no problems! It&#39;s a first and really wonderful. I will get more. This size is a bit small for a main earring so I will get the next size up. THANK YOU.&#34;,\n   &#39;rating&#39;: 5.0,\n   &#39;title&#39;: &#39;No irritation for severe nickle allergy sufferer!&#39;},\n  {&#39;review&#39;: &#39;It is refreshing to wear these earrings because they don\\&#39;t make my earlobes swell up. My ears are too sensitive for &#34;sensitive&#34; earrings and I was about ready to give up and resign myself to clip earrings. Fortunately I found these. They are more expensive than other earrings due to the metal, but well worth it. It\\&#39;s like my ears don\\&#39;t even know they\\&#39;re there :) .&#39;,\n   &#39;rating&#39;: 5.0,\n   &#39;title&#39;: &#39;a relief to wear these&#39;},\n  {&#39;review&#39;: &#34;I just had my ears pierced a little over two months ago and I thought the minor discomfort was part of the healing process.  I decided to try these but honestly didn&#39;t expect there to be a difference.  Within hours of wearing these my ears had no discomfort at all.  Again, my ears weren&#39;t red or swollen but just had a very minor ache.  I&#39;d recommend these to people even if it&#39;s not the last resort to wearing earrings.\\n\\nI also really like the small simple style so these were perfect for me anyway.  I&#39;ve found it very difficult to find earrings I like in stores because they all seem so big and overbearing.  I&#39;m sure that I will end up buying more pairs of these in a similar style and definitely give preference to titanium earrings.&#34;,\n   &#39;rating&#39;: 5.0,\n   &#39;title&#39;: &#39;Pleasantly Surprised&#39;},\n  {&#39;review&#39;: &#34;I just got these a few weeks ago (along with a pair of titanium hoops) and love them. I haven&#39;t been able to wear earrings for years because of nickel allergies, and these earrings haven&#39;t given me any trouble whatsoever!&#34;,\n   &#39;rating&#39;: 5.0,\n   &#39;title&#39;: &#39;I can wear earrings again!&#39;},\n  {&#39;review&#39;: &#39;these are the only earrings that have not caused an allergic reaction and they are very pretty too.&#39;,\n   &#39;rating&#39;: 5.0,\n   &#39;title&#39;: &#39;lovely&#39;},\n  {&#39;review&#39;: &#39;I have only owned these earrings a few weeks, but I\\&#39;m quite pleased with them.  Before buying these, I had been completely unable to wear earrings for many years.  My earlobes would swell, turn red, and bleed immediately after inserting even &#34;hypoallergenic&#34; earrings.  I\\&#39;ve tried other brands, such as Simply Whispers, which I really wanted to love, but I simply couldn\\&#39;t wear them.  I\\&#39;d given up and given away most of my earrings and hadn\\&#39;t bought or tried any earrings for a few years.\\n\\nWhen these first arrived, I had trouble inserting them because my earlobes had almost grown closed, and I traumatized my ears a bit.  Over the next several days, I removed them at night, then irritated my ears again when I reinserted them in the morning.  Finally, I decided to just leave them in and allow my ears to heal, and that has worked just fine.  Now my ears have healed, and I haven\\&#39;t bothered to remove the earrings for a couple of weeks.\\n\\nI noticed that in a review for one of the other styles, the reviewer had two complaints: that the ad appeared misleading in that it looked like you were purchasing three pairs of earrings, and that the 3mm size was tiny.  So take note:  you are ordering just one pair of earrings, and the 3mm (mini) size earrings are very small - take a look at a ruler to get an idea - but they provide an understated bit of sparkle.\\n\\nI highly recommend these for people with metal allergies - I\\&#39;m planning to get another pair.&#39;,\n   &#39;rating&#39;: 5.0,\n   &#39;title&#39;: &#39;Earrings I can wear!&#39;},\n  {&#39;review&#39;: &#39;small enough to fit in my cartilage peircing.&#39;,\n   &#39;rating&#39;: 4.0,\n   &#39;title&#39;: &#39;Four Stars&#39;},\n  {&#39;review&#39;: &#39;Wanted some Titanium earrings that I could sleep in, and not have to keep taking on and off...these are great! Love them!&#39;,\n   &#39;rating&#39;: 5.0,\n   &#39;title&#39;: &#39;Very nice earrings&#39;},\n  {&#39;review&#39;: &#39;They are really nice for the price!  Perfect, made out of titanium for those who have allergies to the metals used in mall piercing places!&#39;,\n   &#39;rating&#39;: 5.0,\n   &#39;title&#39;: &#39;got these for my daughter&#39;},\n  {&#39;review&#39;: &#34;They are a good size for my daughter, but they would be too small for an adult.  My daughter cannot wear metal earrings due to a nickel allergy, but she&#39;s fine with these ones and can leave them in for days.  They are pretty for her.&#34;,\n   &#39;rating&#39;: 5.0,\n   &#39;title&#39;: &#39;Good for a child&#39;},\n  {&#39;review&#39;: &#34;My daughter&#39;s ears were pierced 8 months ago and has only been able to comfortably wear her starter earrings. These are the first earrings that she could change to and didn&#39;t irritate her skin in any way.  They are small but she is 11 so appropriate.&#34;,\n   &#39;rating&#39;: 5.0,\n   &#39;title&#39;: &#39;She can finally change her earrings&#39;},\n  {&#39;review&#39;: &#39;Who knows if these Mini Crystal Silver Titanium Earrings won\\&#39;t lose their finish? I bought some silver hoop earrings from Simply Whispers, and the hoops turned black very quickly--I\\&#39;d only worn them a few times. When I emailed Simply Whispers\\&#39; so-called &#34;customer service&#34; about the problem, I got no response, even though I attached pictures that showed the defect.  After several weeks, I mailed the earrings back to them, with an explanatory letter. I thought if they saw the defect for themselves, they would respond. Again, ZERO response. Once they have your money, you can expect NO response from Simply Whispers if the product proves defective.&#39;,\n   &#39;rating&#39;: 1.0,\n   &#39;title&#39;: &#39;Non-Existent Customer Service&#39;},\n  {&#39;review&#39;: &#39;These are the perfect size for my 1 year old daughter.  No worries about metal allergies and they are very pretty, a perfect everyday earring!&#39;,\n   &#39;rating&#39;: 5.0,\n   &#39;title&#39;: &#39;Perfect&#39;},\n  {&#39;review&#39;: &#34;My ears were red for a couple of hours after putting these in because they weren&#39;t use to wearing earrings.  I was actually surprised I could get these earrings in because it had been so long since I wore any due to a reaction to every pair I ever tried. These are nice, subtle, sparkly ear decorations and they don&#39;t make my ears ooze, swell or glow red.  Love them.  Love that I can wear earrings.&#34;,\n   &#39;rating&#39;: 5.0,\n   &#39;title&#39;: &#34;I&#39;m so glad I can finally wear earrings!&#34;},\n  {&#39;review&#39;: &#34;I ordered these knowing full well that they were small and 3mm. They arrived beautifully packaged, sterile, and upon putting them in earring holes that cannot even tolerate 14K solid gold, I felt nothing. I mean, I felt NO irritation, burning, redness, warmth, and I love them. The backs are titanium too, which makes them non-allergenic also. I had a piercing done with titanium earrings (which turned out to be anodized (plated) titanium over surgical steel which had surgical steel (not titanium) backs. The backs irritated my ears immediately. The plating wore off fast while using hydrogen peroxide and antibacterial ointment on the earrings and holes (as they were irritated).\\n\\nWhat a relief to find these earrings. I have now ordered the Blomdahl sleeper hoops 12mm as well and just waiting for those to arrive. I wear these 3mm studs in my upper ear cartilage piercing and lower lobe 2nd piercing. I love the size because I don&#39;t go for big earrings.\\n\\nI strongly recommend these. And just to let others know who are considering other titanium earrings, even grade 23 titanium irritated the heck out of my ears. The Blomdahl earrings are made of grade 1-2 titanium which has NO ALLOYS and therefore do not create allergies. Other titanium used in earrings (grade 23 and grade 5) is listed as hypo-allergenic, but it has additional metals and alloys which can cause sensitivity issues.&#34;,\n   &#39;rating&#39;: 5.0,\n   &#39;title&#39;: &#39;Worth the cost and not looking  back...&#39;},\n  {&#39;review&#39;: &#39;I bought this pair of earrings because I felt my previously pierced ears were starting to close (I only wore earrings sporadically over the last 2 years), and wearing cheap metals were irritating them.  So I bought this pair mainly for the titanium, so my ears can heal.  They did not irritate at all, I wore them continuously for 3 to 4 weeks.  Now I can wear any earring without problem.  I still wear these to bed.&#39;,\n   &#39;rating&#39;: 5.0,\n   &#39;title&#39;: &#39;Did its job.&#39;}]}</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Akul\npipeline = [\n    { \"$match\": { \"ASIN\": \"B00015VKT4\" } },\n    { \"$unwind\": \"$reviews\" },\n    { \"$group\": { \"_id\": \"$reviews.rating\", \"count\": { \"$sum\": 1 } } }\n]\n\nprojection = {\n    \"ASIN\": 1,\n    \"reviews\": 1\n             }\n      \npipeline = [\n    {\"$unwind\": \"$reviews\"},\n    {\"$project\": projection}\n]\n\nresults = amazon_collection.aggregate(pipeline)\n\n\n\ndata = []    \nfor doc in results:\n    _id = doc['_id']\n    ASIN = doc['ASIN']\n    reviews = doc['reviews']['review']\n    rating = doc['reviews']['rating']\n    title = doc['reviews']['title']\n    data.append([_id, ASIN, reviews, rating, title])\n\ndf = pd.DataFrame(data, columns=['_id', 'ASIN', 'reviews', 'rating', 'title'])\ndf = df.head(95000)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"cad74bce-385f-4467-978e-ac3b2861be58","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Define the threshold for a \"good\" review\nthreshold = 4\n\n# Convert the reviews and title columns into bag-of-words features\nvectorizer = CountVectorizer()\nX = vectorizer.fit_transform(df['reviews'] + ' ' + df['title'])\n\n# Convert the sparse matrix X to a dense numpy array\nX = X.toarray()\n\n# Create the target variable\ny = df['rating'].apply(lambda x: 1 if x >= threshold else 0)\n\n# Convert the pandas series y to a numpy array\ny = y.values\n\n# Define the PyTorch dataset and dataloader\nclass ReviewDataset(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.float32)\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx]\n\ndataset = ReviewDataset(X, y)\ndataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n\n# Define the logistic regression model using PyTorch\nclass LogisticRegressionModel(nn.Module):\n    def __init__(self, input_size):\n        super(LogisticRegressionModel, self).__init__()\n        self.linear = nn.Linear(input_size, 1)\n\n    def forward(self, x):\n        return torch.sigmoid(self.linear(x))\n\nmodel = LogisticRegressionModel(X.shape[1])\n\n# Define the loss function and optimizer\ncriterion = nn.L1Loss()\noptimizer = optim.Adam(model.parameters())\n\n# Train the model using PyTorch\nfor epoch in range(5):\n    running_loss = 0.0\n    for i, (inputs, labels) in enumerate(dataloader):\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels.unsqueeze(1))\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n    print('Epoch %d Loss: %.3f' % (epoch + 1, running_loss / len(dataloader)))\n\n# Evaluate the model\nwith torch.no_grad():\n    inputs = torch.tensor(X, dtype=torch.float32)\n    labels = torch.tensor(y, dtype=torch.float32)\n    outputs = model(inputs)\n    predictions = outputs.squeeze().cpu().numpy()\n    mae = np.mean(np.abs(predictions - y))\n    print('MAE:', mae)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"a5a975e4-d528-40fa-90a2-e1dabb28bb93","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Epoch 1 Loss: 0.207\nEpoch 2 Loss: 0.111\nEpoch 3 Loss: 0.091\nEpoch 4 Loss: 0.082\nEpoch 5 Loss: 0.077\nMAE: 0.07339412879419072\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Epoch 1 Loss: 0.207\nEpoch 2 Loss: 0.111\nEpoch 3 Loss: 0.091\nEpoch 4 Loss: 0.082\nEpoch 5 Loss: 0.077\nMAE: 0.07339412879419072\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Define the input example\ninput_example = vectorizer.transform([\"this item is a piece of junk, I hate it\"]).toarray()\n\n# Convert the input example to a PyTorch tensor\ninput_tensor = torch.tensor(input_example, dtype=torch.float32)\n\n# Make the prediction using the trained model\nwith torch.no_grad():\n    output_tensor = model(input_tensor)\n    prediction = output_tensor.item()\n\n# Print the prediction\nprint(\"Prediction:\", prediction)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"79344d4e-8ee2-481e-a84f-3d1966c8701a","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Prediction: 0.09791531413793564\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Prediction: 0.09791531413793564\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Define the input example\ninput_example = vectorizer.transform([\"Great! I love it!\"]).toarray()\n\n# Convert the input example to a PyTorch tensor\ninput_tensor = torch.tensor(input_example, dtype=torch.float32)\n\n# Make the prediction using the trained model\nwith torch.no_grad():\n    output_tensor = model(input_tensor)\n    prediction = output_tensor.item()\n\n# Print the prediction\nprint(\"Prediction:\", prediction)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"ab21b978-8ced-4507-a52c-7f34d385f0af","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Prediction: 0.9960452914237976\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Prediction: 0.9960452914237976\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Patricia\n# https://spacy.io/universe/project/eng_spacysentiment\n!pip install eng-spacysentiment\nimport eng_spacysentiment\nnlp = eng_spacysentiment.load()\ntext = \"Welcome to Arsenals official YouTube channel Watch as we take you closer and show you the personality of the club\"\ndoc = nlp(text)\nprint(doc.cats)\nrating_pred = doc.cats['positive']\nprint(rating_pred)\n# label = amazon_review/5\n# error = rating_pred - amazon_label\n\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"8558db63-1a78-43f7-83cd-1d3e759b8bdb","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">{&#39;positive&#39;: 0.9402117729187012, &#39;negative&#39;: 0.05978821963071823}\n0.9402117729187012\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">{&#39;positive&#39;: 0.9402117729187012, &#39;negative&#39;: 0.05978821963071823}\n0.9402117729187012\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Patricia\nrow = 8\ntext = df['reviews'][row]\nprint(text)\ndoc = nlp(text)\nrating_pred = doc.cats['positive']\nprint(rating_pred)\nlabel = df['rating'][row]/5\nprint(label)\nerror = rating_pred - label\nprint(error)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"653cb1b7-05e8-4d52-8427-8c18621c389c","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">They are really nice for the price!  Perfect, made out of titanium for those who have allergies to the metals used in mall piercing places!\n0.9996743202209473\n1.0\n-0.0003256797790527344\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">They are really nice for the price!  Perfect, made out of titanium for those who have allergies to the metals used in mall piercing places!\n0.9996743202209473\n1.0\n-0.0003256797790527344\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Patricia\ndf['rating_perc'] = df['rating']/5\ndf['review_nlp'] = df['reviews'].apply(nlp)\ndf['spacy_prediction'] = df['review_nlp'].apply(lambda row : row.cats['positive'])\ndf['spacy_error'] = abs(df['rating_perc']-df['spacy_prediction'])\nmae = df['spacy_error'].mean()\nmae # 0.31524060 "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"a1899677-ebec-4e67-8a03-b13b40ae0f7e","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[9]: 0.3153731920527474</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[9]: 0.3153731920527474</div>"]}}],"execution_count":0},{"cell_type":"code","source":["df[df['rating_perc']!= 1].head(4)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"3200df78-5bcb-4299-ac01-7a575d4012cb","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# VMK\n\nfrom transformers import pipeline\n# sentiment_pipeline = pipeline(\"sentiment-analysis\")\n# sentiment_pipeline = pipeline(model=\"finiteautomata/bertweet-base-sentiment-analysis\")\nsentiment_pipeline = pipeline(\"sentiment-analysis\",model=\"siebert/sentiment-roberta-large-english\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"2b4d69bf-242c-4cad-abb3-b20a85693019","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">\rDownloading (…)lve/main/config.json:   0%|          | 0.00/687 [00:00&lt;?, ?B/s]\rDownloading (…)lve/main/config.json: 100%|██████████| 687/687 [00:00&lt;00:00, 225kB/s]\n\rDownloading pytorch_model.bin:   0%|          | 0.00/1.42G [00:00&lt;?, ?B/s]\rDownloading pytorch_model.bin:   1%|▏         | 21.0M/1.42G [00:00&lt;00:09, 149MB/s]\rDownloading pytorch_model.bin:   3%|▎         | 41.9M/1.42G [00:00&lt;00:13, 99.2MB/s]\rDownloading pytorch_model.bin:   4%|▍         | 62.9M/1.42G [00:00&lt;00:14, 93.3MB/s]\rDownloading pytorch_model.bin:   6%|▌         | 83.9M/1.42G [00:00&lt;00:14, 90.8MB/s]\rDownloading pytorch_model.bin:   7%|▋         | 105M/1.42G [00:01&lt;00:12, 109MB/s]  \rDownloading pytorch_model.bin:   9%|▉         | 126M/1.42G [00:01&lt;00:10, 120MB/s]\rDownloading pytorch_model.bin:  10%|█         | 147M/1.42G [00:01&lt;00:09, 131MB/s]\rDownloading pytorch_model.bin:  12%|█▏        | 168M/1.42G [00:01&lt;00:11, 106MB/s]\rDownloading pytorch_model.bin:  13%|█▎        | 189M/1.42G [00:01&lt;00:12, 103MB/s]\rDownloading pytorch_model.bin:  15%|█▍        | 210M/1.42G [00:01&lt;00:11, 107MB/s]\rDownloading pytorch_model.bin:  16%|█▌        | 231M/1.42G [00:02&lt;00:11, 100MB/s]\rDownloading pytorch_model.bin:  18%|█▊        | 252M/1.42G [00:02&lt;00:11, 104MB/s]\rDownloading pytorch_model.bin:  19%|█▉        | 273M/1.42G [00:02&lt;00:11, 99.0MB/s]\rDownloading pytorch_model.bin:  20%|█▉        | 283M/1.42G [00:02&lt;00:11, 97.1MB/s]\rDownloading pytorch_model.bin:  21%|██        | 294M/1.42G [00:02&lt;00:13, 83.7MB/s]\rDownloading pytorch_model.bin:  22%|██▏       | 315M/1.42G [00:03&lt;00:11, 93.5MB/s]\rDownloading pytorch_model.bin:  23%|██▎       | 325M/1.42G [00:03&lt;00:12, 89.4MB/s]\rDownloading pytorch_model.bin:  24%|██▎       | 336M/1.42G [00:03&lt;00:11, 91.7MB/s]\rDownloading pytorch_model.bin:  25%|██▌       | 357M/1.42G [00:03&lt;00:10, 104MB/s] \rDownloading pytorch_model.bin:  26%|██▌       | 367M/1.42G [00:03&lt;00:10, 96.3MB/s]\rDownloading pytorch_model.bin:  27%|██▋       | 377M/1.42G [00:03&lt;00:15, 67.3MB/s]\rDownloading pytorch_model.bin:  28%|██▊       | 398M/1.42G [00:04&lt;00:11, 90.3MB/s]\rDownloading pytorch_model.bin:  30%|██▉       | 419M/1.42G [00:04&lt;00:09, 111MB/s] \rDownloading pytorch_model.bin:  31%|███       | 440M/1.42G [00:04&lt;00:07, 129MB/s]\rDownloading pytorch_model.bin:  32%|███▏      | 461M/1.42G [00:04&lt;00:06, 140MB/s]\rDownloading pytorch_model.bin:  34%|███▍      | 482M/1.42G [00:04&lt;00:08, 117MB/s]\rDownloading pytorch_model.bin:  35%|███▌      | 503M/1.42G [00:04&lt;00:07, 119MB/s]\rDownloading pytorch_model.bin:  37%|███▋      | 524M/1.42G [00:04&lt;00:06, 135MB/s]\rDownloading pytorch_model.bin:  38%|███▊      | 545M/1.42G [00:05&lt;00:06, 126MB/s]\rDownloading pytorch_model.bin:  40%|███▉      | 566M/1.42G [00:05&lt;00:06, 140MB/s]\rDownloading pytorch_model.bin:  41%|████▏     | 587M/1.42G [00:05&lt;00:06, 139MB/s]\rDownloading pytorch_model.bin:  43%|████▎     | 608M/1.42G [00:05&lt;00:05, 147MB/s]\rDownloading pytorch_model.bin:  44%|████▍     | 629M/1.42G [00:05&lt;00:05, 142MB/s]\rDownloading pytorch_model.bin:  46%|████▌     | 650M/1.42G [00:05&lt;00:05, 146MB/s]\rDownloading pytorch_model.bin:  47%|████▋     | 671M/1.42G [00:05&lt;00:05, 147MB/s]\rDownloading pytorch_model.bin:  49%|████▊     | 692M/1.42G [00:06&lt;00:05, 144MB/s]\rDownloading pytorch_model.bin:  50%|█████     | 713M/1.42G [00:06&lt;00:04, 149MB/s]\rDownloading pytorch_model.bin:  52%|█████▏    | 734M/1.42G [00:06&lt;00:05, 136MB/s]\rDownloading pytorch_model.bin:  53%|█████▎    | 755M/1.42G [00:06&lt;00:04, 136MB/s]\rDownloading pytorch_model.bin:  55%|█████▍    | 776M/1.42G [00:06&lt;00:05, 129MB/s]\rDownloading pytorch_model.bin:  56%|█████▌    | 797M/1.42G [00:06&lt;00:04, 141MB/s]\rDownloading pytorch_model.bin:  58%|█████▊    | 818M/1.42G [00:07&lt;00:04, 142MB/s]\rDownloading pytorch_model.bin:  59%|█████▉    | 839M/1.42G [00:07&lt;00:03, 153MB/s]\rDownloading pytorch_model.bin:  60%|██████    | 860M/1.42G [00:07&lt;00:03, 158MB/s]\rDownloading pytorch_model.bin:  62%|██████▏   | 881M/1.42G [00:07&lt;00:04, 131MB/s]\rDownloading pytorch_model.bin:  63%|██████▎   | 902M/1.42G [00:07&lt;00:04, 129MB/s]\rDownloading pytorch_model.bin:  66%|██████▌   | 933M/1.42G [00:07&lt;00:03, 149MB/s]\rDownloading pytorch_model.bin:  68%|██████▊   | 965M/1.42G [00:07&lt;00:02, 164MB/s]\rDownloading pytorch_model.bin:  69%|██████▉   | 986M/1.42G [00:08&lt;00:02, 163MB/s]\rDownloading pytorch_model.bin:  71%|███████   | 1.01G/1.42G [00:08&lt;00:02, 156MB/s]\rDownloading pytorch_model.bin:  72%|███████▏  | 1.03G/1.42G [00:08&lt;00:02, 145MB/s]\rDownloading pytorch_model.bin:  74%|███████▍  | 1.05G/1.42G [00:08&lt;00:02, 135MB/s]\rDownloading pytorch_model.bin:  75%|███████▌  | 1.07G/1.42G [00:08&lt;00:02, 129MB/s]\rDownloading pytorch_model.bin:  77%|███████▋  | 1.09G/1.42G [00:08&lt;00:02, 128MB/s]\rDownloading pytorch_model.bin:  78%|███████▊  | 1.11G/1.42G [00:09&lt;00:02, 131MB/s]\rDownloading pytorch_model.bin:  80%|███████▉  | 1.13G/1.42G [00:09&lt;00:02, 112MB/s]\rDownloading pytorch_model.bin:  81%|████████  | 1.15G/1.42G [00:09&lt;00:02, 114MB/s]\rDownloading pytorch_model.bin:  83%|████████▎ | 1.17G/1.42G [00:09&lt;00:02, 114MB/s]\rDownloading pytorch_model.bin:  84%|████████▍ | 1.20G/1.42G [00:09&lt;00:01, 118MB/s]\rDownloading pytorch_model.bin:  86%|████████▌ | 1.22G/1.42G [00:10&lt;00:01, 125MB/s]\rDownloading pytorch_model.bin:  87%|████████▋ | 1.24G/1.42G [00:10&lt;00:01, 114MB/s]\rDownloading pytorch_model.bin:  89%|████████▊ | 1.26G/1.42G [00:10&lt;00:01, 121MB/s]\rDownloading pytorch_model.bin:  90%|████████▉ | 1.28G/1.42G [00:10&lt;00:01, 97.8MB/s]\rDownloading pytorch_model.bin:  91%|█████████▏| 1.30G/1.42G [00:10&lt;00:01, 91.2MB/s]\rDownloading pytorch_model.bin:  92%|█████████▏| 1.31G/1.42G [00:11&lt;00:01, 89.1MB/s]\rDownloading pytorch_model.bin:  94%|█████████▎| 1.33G/1.42G [00:11&lt;00:01, 89.9MB/s]\rDownloading pytorch_model.bin:  95%|█████████▌| 1.35G/1.42G [00:11&lt;00:00, 105MB/s] \rDownloading pytorch_model.bin:  97%|█████████▋| 1.37G/1.42G [00:11&lt;00:00, 105MB/s]\rDownloading pytorch_model.bin:  98%|█████████▊| 1.39G/1.42G [00:11&lt;00:00, 103MB/s]\rDownloading pytorch_model.bin: 100%|█████████▉| 1.42G/1.42G [00:12&lt;00:00, 98.1MB/s]\rDownloading pytorch_model.bin: 100%|██████████| 1.42G/1.42G [00:12&lt;00:00, 116MB/s] \n\rDownloading (…)okenizer_config.json:   0%|          | 0.00/256 [00:00&lt;?, ?B/s]\rDownloading (…)okenizer_config.json: 100%|██████████| 256/256 [00:00&lt;00:00, 77.8kB/s]\n\rDownloading (…)olve/main/vocab.json:   0%|          | 0.00/798k [00:00&lt;?, ?B/s]\rDownloading (…)olve/main/vocab.json: 100%|██████████| 798k/798k [00:00&lt;00:00, 7.91MB/s]\rDownloading (…)olve/main/vocab.json: 100%|██████████| 798k/798k [00:00&lt;00:00, 7.81MB/s]\n\rDownloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00&lt;?, ?B/s]\rDownloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00&lt;00:00, 8.72MB/s]\n\rDownloading (…)cial_tokens_map.json:   0%|          | 0.00/150 [00:00&lt;?, ?B/s]\rDownloading (…)cial_tokens_map.json: 100%|██████████| 150/150 [00:00&lt;00:00, 42.5kB/s]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">\rDownloading (…)lve/main/config.json:   0%|          | 0.00/687 [00:00&lt;?, ?B/s]\rDownloading (…)lve/main/config.json: 100%|██████████| 687/687 [00:00&lt;00:00, 225kB/s]\n\rDownloading pytorch_model.bin:   0%|          | 0.00/1.42G [00:00&lt;?, ?B/s]\rDownloading pytorch_model.bin:   1%|▏         | 21.0M/1.42G [00:00&lt;00:09, 149MB/s]\rDownloading pytorch_model.bin:   3%|▎         | 41.9M/1.42G [00:00&lt;00:13, 99.2MB/s]\rDownloading pytorch_model.bin:   4%|▍         | 62.9M/1.42G [00:00&lt;00:14, 93.3MB/s]\rDownloading pytorch_model.bin:   6%|▌         | 83.9M/1.42G [00:00&lt;00:14, 90.8MB/s]\rDownloading pytorch_model.bin:   7%|▋         | 105M/1.42G [00:01&lt;00:12, 109MB/s]  \rDownloading pytorch_model.bin:   9%|▉         | 126M/1.42G [00:01&lt;00:10, 120MB/s]\rDownloading pytorch_model.bin:  10%|█         | 147M/1.42G [00:01&lt;00:09, 131MB/s]\rDownloading pytorch_model.bin:  12%|█▏        | 168M/1.42G [00:01&lt;00:11, 106MB/s]\rDownloading pytorch_model.bin:  13%|█▎        | 189M/1.42G [00:01&lt;00:12, 103MB/s]\rDownloading pytorch_model.bin:  15%|█▍        | 210M/1.42G [00:01&lt;00:11, 107MB/s]\rDownloading pytorch_model.bin:  16%|█▌        | 231M/1.42G [00:02&lt;00:11, 100MB/s]\rDownloading pytorch_model.bin:  18%|█▊        | 252M/1.42G [00:02&lt;00:11, 104MB/s]\rDownloading pytorch_model.bin:  19%|█▉        | 273M/1.42G [00:02&lt;00:11, 99.0MB/s]\rDownloading pytorch_model.bin:  20%|█▉        | 283M/1.42G [00:02&lt;00:11, 97.1MB/s]\rDownloading pytorch_model.bin:  21%|██        | 294M/1.42G [00:02&lt;00:13, 83.7MB/s]\rDownloading pytorch_model.bin:  22%|██▏       | 315M/1.42G [00:03&lt;00:11, 93.5MB/s]\rDownloading pytorch_model.bin:  23%|██▎       | 325M/1.42G [00:03&lt;00:12, 89.4MB/s]\rDownloading pytorch_model.bin:  24%|██▎       | 336M/1.42G [00:03&lt;00:11, 91.7MB/s]\rDownloading pytorch_model.bin:  25%|██▌       | 357M/1.42G [00:03&lt;00:10, 104MB/s] \rDownloading pytorch_model.bin:  26%|██▌       | 367M/1.42G [00:03&lt;00:10, 96.3MB/s]\rDownloading pytorch_model.bin:  27%|██▋       | 377M/1.42G [00:03&lt;00:15, 67.3MB/s]\rDownloading pytorch_model.bin:  28%|██▊       | 398M/1.42G [00:04&lt;00:11, 90.3MB/s]\rDownloading pytorch_model.bin:  30%|██▉       | 419M/1.42G [00:04&lt;00:09, 111MB/s] \rDownloading pytorch_model.bin:  31%|███       | 440M/1.42G [00:04&lt;00:07, 129MB/s]\rDownloading pytorch_model.bin:  32%|███▏      | 461M/1.42G [00:04&lt;00:06, 140MB/s]\rDownloading pytorch_model.bin:  34%|███▍      | 482M/1.42G [00:04&lt;00:08, 117MB/s]\rDownloading pytorch_model.bin:  35%|███▌      | 503M/1.42G [00:04&lt;00:07, 119MB/s]\rDownloading pytorch_model.bin:  37%|███▋      | 524M/1.42G [00:04&lt;00:06, 135MB/s]\rDownloading pytorch_model.bin:  38%|███▊      | 545M/1.42G [00:05&lt;00:06, 126MB/s]\rDownloading pytorch_model.bin:  40%|███▉      | 566M/1.42G [00:05&lt;00:06, 140MB/s]\rDownloading pytorch_model.bin:  41%|████▏     | 587M/1.42G [00:05&lt;00:06, 139MB/s]\rDownloading pytorch_model.bin:  43%|████▎     | 608M/1.42G [00:05&lt;00:05, 147MB/s]\rDownloading pytorch_model.bin:  44%|████▍     | 629M/1.42G [00:05&lt;00:05, 142MB/s]\rDownloading pytorch_model.bin:  46%|████▌     | 650M/1.42G [00:05&lt;00:05, 146MB/s]\rDownloading pytorch_model.bin:  47%|████▋     | 671M/1.42G [00:05&lt;00:05, 147MB/s]\rDownloading pytorch_model.bin:  49%|████▊     | 692M/1.42G [00:06&lt;00:05, 144MB/s]\rDownloading pytorch_model.bin:  50%|█████     | 713M/1.42G [00:06&lt;00:04, 149MB/s]\rDownloading pytorch_model.bin:  52%|█████▏    | 734M/1.42G [00:06&lt;00:05, 136MB/s]\rDownloading pytorch_model.bin:  53%|█████▎    | 755M/1.42G [00:06&lt;00:04, 136MB/s]\rDownloading pytorch_model.bin:  55%|█████▍    | 776M/1.42G [00:06&lt;00:05, 129MB/s]\rDownloading pytorch_model.bin:  56%|█████▌    | 797M/1.42G [00:06&lt;00:04, 141MB/s]\rDownloading pytorch_model.bin:  58%|█████▊    | 818M/1.42G [00:07&lt;00:04, 142MB/s]\rDownloading pytorch_model.bin:  59%|█████▉    | 839M/1.42G [00:07&lt;00:03, 153MB/s]\rDownloading pytorch_model.bin:  60%|██████    | 860M/1.42G [00:07&lt;00:03, 158MB/s]\rDownloading pytorch_model.bin:  62%|██████▏   | 881M/1.42G [00:07&lt;00:04, 131MB/s]\rDownloading pytorch_model.bin:  63%|██████▎   | 902M/1.42G [00:07&lt;00:04, 129MB/s]\rDownloading pytorch_model.bin:  66%|██████▌   | 933M/1.42G [00:07&lt;00:03, 149MB/s]\rDownloading pytorch_model.bin:  68%|██████▊   | 965M/1.42G [00:07&lt;00:02, 164MB/s]\rDownloading pytorch_model.bin:  69%|██████▉   | 986M/1.42G [00:08&lt;00:02, 163MB/s]\rDownloading pytorch_model.bin:  71%|███████   | 1.01G/1.42G [00:08&lt;00:02, 156MB/s]\rDownloading pytorch_model.bin:  72%|███████▏  | 1.03G/1.42G [00:08&lt;00:02, 145MB/s]\rDownloading pytorch_model.bin:  74%|███████▍  | 1.05G/1.42G [00:08&lt;00:02, 135MB/s]\rDownloading pytorch_model.bin:  75%|███████▌  | 1.07G/1.42G [00:08&lt;00:02, 129MB/s]\rDownloading pytorch_model.bin:  77%|███████▋  | 1.09G/1.42G [00:08&lt;00:02, 128MB/s]\rDownloading pytorch_model.bin:  78%|███████▊  | 1.11G/1.42G [00:09&lt;00:02, 131MB/s]\rDownloading pytorch_model.bin:  80%|███████▉  | 1.13G/1.42G [00:09&lt;00:02, 112MB/s]\rDownloading pytorch_model.bin:  81%|████████  | 1.15G/1.42G [00:09&lt;00:02, 114MB/s]\rDownloading pytorch_model.bin:  83%|████████▎ | 1.17G/1.42G [00:09&lt;00:02, 114MB/s]\rDownloading pytorch_model.bin:  84%|████████▍ | 1.20G/1.42G [00:09&lt;00:01, 118MB/s]\rDownloading pytorch_model.bin:  86%|████████▌ | 1.22G/1.42G [00:10&lt;00:01, 125MB/s]\rDownloading pytorch_model.bin:  87%|████████▋ | 1.24G/1.42G [00:10&lt;00:01, 114MB/s]\rDownloading pytorch_model.bin:  89%|████████▊ | 1.26G/1.42G [00:10&lt;00:01, 121MB/s]\rDownloading pytorch_model.bin:  90%|████████▉ | 1.28G/1.42G [00:10&lt;00:01, 97.8MB/s]\rDownloading pytorch_model.bin:  91%|█████████▏| 1.30G/1.42G [00:10&lt;00:01, 91.2MB/s]\rDownloading pytorch_model.bin:  92%|█████████▏| 1.31G/1.42G [00:11&lt;00:01, 89.1MB/s]\rDownloading pytorch_model.bin:  94%|█████████▎| 1.33G/1.42G [00:11&lt;00:01, 89.9MB/s]\rDownloading pytorch_model.bin:  95%|█████████▌| 1.35G/1.42G [00:11&lt;00:00, 105MB/s] \rDownloading pytorch_model.bin:  97%|█████████▋| 1.37G/1.42G [00:11&lt;00:00, 105MB/s]\rDownloading pytorch_model.bin:  98%|█████████▊| 1.39G/1.42G [00:11&lt;00:00, 103MB/s]\rDownloading pytorch_model.bin: 100%|█████████▉| 1.42G/1.42G [00:12&lt;00:00, 98.1MB/s]\rDownloading pytorch_model.bin: 100%|██████████| 1.42G/1.42G [00:12&lt;00:00, 116MB/s] \n\rDownloading (…)okenizer_config.json:   0%|          | 0.00/256 [00:00&lt;?, ?B/s]\rDownloading (…)okenizer_config.json: 100%|██████████| 256/256 [00:00&lt;00:00, 77.8kB/s]\n\rDownloading (…)olve/main/vocab.json:   0%|          | 0.00/798k [00:00&lt;?, ?B/s]\rDownloading (…)olve/main/vocab.json: 100%|██████████| 798k/798k [00:00&lt;00:00, 7.91MB/s]\rDownloading (…)olve/main/vocab.json: 100%|██████████| 798k/798k [00:00&lt;00:00, 7.81MB/s]\n\rDownloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00&lt;?, ?B/s]\rDownloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00&lt;00:00, 8.72MB/s]\n\rDownloading (…)cial_tokens_map.json:   0%|          | 0.00/150 [00:00&lt;?, ?B/s]\rDownloading (…)cial_tokens_map.json: 100%|██████████| 150/150 [00:00&lt;00:00, 42.5kB/s]\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# VMK\n\ndef m_bert(i):\n    try:\n        i = sentiment_pipeline(str(i))[0]\n        s = i['score']/.4\n        if i['label'] == 'POSITIVE':\n            s += 2.5\n        else:\n            s = 2.5-s\n        return s\n    except:\n         return 2.5"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"dbe9ff8c-d757-4b08-bfc7-7d0f66f47d5d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# VMK\n\ndf_s = df.head(10).copy()\n\ndf_s['m_prediction'] = df_s['reviews'].apply(m_bert)\ndf_s['m_error'] = abs(df_s['rating']-df_s['m_prediction'])\nmae = df_s['m_error'].mean()\n\nprint(df_s.head(2).T)\n\nmae #for the whole dataset: 0.795"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"0d82900e-0fe9-4efc-9a70-4b28b87b21b3","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"ce4bb8c1-7d54-4832-9591-37e60d0503cf","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Final_Project","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":88578087200992}},"nbformat":4,"nbformat_minor":0}
